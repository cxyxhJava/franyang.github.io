---
layout:     post
title:      Hadoop
subtitle:   Hadoop入门第二课
date:       2019-08-24
author:     程序猿小哈
header-img: img/db.jpeg
catalog: 	true
tags:
    - 大数据
---

### Hadoop02-安装

#### Hadoop运行环境搭

#### 1. 虚拟机+linux环境搭建
#### 2. 下载安装jdk hadoop是java 
#### 3. ssh设置免密登陆
#### 4. hosts文件修改配置域名
#### 5. 关闭防火墙/开放端口  50070(hadoop端口)
#### 6. 下载hadoop 
**[下载地址](https://archive.apache.org/dist/hadoop/common/)**
![hadoop下载地址](/postImg/Hadoop01.jpg )
#### 7. 将Hadoop添加到环境变量<br>

   ```
   7.1 /etc/profile 上添加Hadoop地址  
      export HADOOP_HOME=/frank/hadoop-3.2.0(自己的Hadoop安装地址)
      export PATH=$PATH:$HADOOP_HOME/bin
      export PATH=$PATH:$HADOOP_HOME/sbin
   7.2 source /etc/profile 启用环境变量<br>
   7.3 hadoop version 查看是否配置成功
   ``` 
#### 8. Hadoop运行 <br>
  8.1 Local(Standalone) Mode 本地(独立)模式 <br>
  8.2 Pseudo-Distributed Mode 伪分布模式 <br>
  8.3 Fully-Distributed Mode 全分布模式 <br>
  
##### 8.1 Local(Standalone) Mode 本地(独立)模式 

###### 8.1.1 使用`grep方式运行` 
```
#创建输入文件
  $ mkdir input
  #拷贝etc/hadoop/下的xml文件到input中 
   $ cp etc/hadoop/*.xml input
   #运行 hadoop-mapreduce-examples-3.2.0.jar
   $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar grep input output 'dfs[a-z.]+'
   #查看运行结果
   $ cat output/*

```
使用input中的的数据运行examples代码,得到图示结果:
![运行结果](/postImg/hadoop/Hadoop02-1.jpg)
![运行结果](/postImg/hadoop/Hadoop02-2.jpg)

###### 8.1.2 使用`wordCount方式`运行

```
 $ mkdir wcinput
 #生成一个文件 
 $ touch wcinput/test.input
 #使用VI/VIM编辑器在文件中录入数据  (我录入的数据为  frank frank  小哈 小哈 小哈 test)
 # 使用wordCount方式传入/传出数据
 $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.0.jar wordcount wcinput wcout
 #查看运行结果
 $ cat wcout/*

``` 
![运行结果](/postImg/hadoop/Hadoop02-3.jpg)
![运行结果](/postImg/hadoop/Hadoop02-4.jpg)

  
##### 8.2 Pseudo-Distributed Mode 伪分布模式

###### 8.2.1 修改配置文件etc/hadoop/下
 ![所有配置文件](/postImg/hadoop/Hadoop02-5.jpg)
 
 `官方配置字段文档详解`
 ![配置文件示例](/postImg/hadoop/Hadoop02-6.jpg)
 
 8.2.1.1 core-site.xml
 
```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://frank:9001</value> --对应的hosts文件内域名  -->hadoop默认为file协议改为hdfs后本地(独立)模式将会不可用
    </property>
	 <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop_repo</value>  临时文件目录设定
    </property>
</configuration>
```
 8.2.1.2 hdfs-site.xml
```
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/frank/hadoop/hdfs/name</value>
        <description>namenode上存储hdfs名字空间元数据 </description> 
    </property>
    
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/frank/hadoop/hdfs/data</value>
        <description>datanode上数据块的物理存储位置</description>
    </property>
    
    <property>
        <name>dfs.replication</name>
        <value>1</value>
        <description>副本个数,配置默认是3,应小于datanode机器数量</description>
    </property>
</configuration>
``` 
 ###### 8.2.2 启动集群
 ```
  # 格式化NameNode(第一次启动格式化,以后不要)
  $ bin/hdfs namenode -format
  # 启动hdfs
  $ sbin/start-dfs.sh
  启动日志存放路径 $HADOOP_LOG_DIR (默认 $HADOOP_HOME/logs).
```
###### 8.2.3 浏览页面

路径: http://localhost:50070/

###### 8.2.4 创建执行MapReduce作业所需的HDFS目录
```
  $ bin/hdfs dfs -mkdir /user
  $ bin/hdfs dfs -mkdir /user/<username>
```



 
 
 

 





 




  
  
  


  

  